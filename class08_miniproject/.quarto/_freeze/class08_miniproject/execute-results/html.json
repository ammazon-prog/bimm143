{
  "hash": "9de0f4f1707a8d2e80dd20752cb8dce4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Class 08: Breast Cancer Analysis Mini Project\"\nauthor: \"Ashley Mazon (PID:A17478903)\"\nformat: html\ntoc: TRUE\n---\n\n## Background \n\nThe goal of this mini-project is for you to explore a complete analysis using the unsupervised learning techniques covered in class. You’ll extend what you’ve learned by combining PCA as a preprocessing step to clustering using data that consist of measurements of cell nuclei of human breast masses. This expands on our RNA-Seq analysis from last day.\n\nThe data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.\n\nValues in this data set describe characteristics of the cell nuclei present in digitized images of a fine needle aspiration (FNA) of a breast mass.\n\n## Data Import \n\nInput data file is saved into my Project directory as a CSV\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.df <- read.csv(\"WisconsinCancer.csv\", row.names = 1)\nhead(wisc.df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         diagnosis radius_mean texture_mean perimeter_mean area_mean\n842302           M       17.99        10.38         122.80    1001.0\n842517           M       20.57        17.77         132.90    1326.0\n84300903         M       19.69        21.25         130.00    1203.0\n84348301         M       11.42        20.38          77.58     386.1\n84358402         M       20.29        14.34         135.10    1297.0\n843786           M       12.45        15.70          82.57     477.1\n         smoothness_mean compactness_mean concavity_mean concave.points_mean\n842302           0.11840          0.27760         0.3001             0.14710\n842517           0.08474          0.07864         0.0869             0.07017\n84300903         0.10960          0.15990         0.1974             0.12790\n84348301         0.14250          0.28390         0.2414             0.10520\n84358402         0.10030          0.13280         0.1980             0.10430\n843786           0.12780          0.17000         0.1578             0.08089\n         symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se\n842302          0.2419                0.07871    1.0950     0.9053        8.589\n842517          0.1812                0.05667    0.5435     0.7339        3.398\n84300903        0.2069                0.05999    0.7456     0.7869        4.585\n84348301        0.2597                0.09744    0.4956     1.1560        3.445\n84358402        0.1809                0.05883    0.7572     0.7813        5.438\n843786          0.2087                0.07613    0.3345     0.8902        2.217\n         area_se smoothness_se compactness_se concavity_se concave.points_se\n842302    153.40      0.006399        0.04904      0.05373           0.01587\n842517     74.08      0.005225        0.01308      0.01860           0.01340\n84300903   94.03      0.006150        0.04006      0.03832           0.02058\n84348301   27.23      0.009110        0.07458      0.05661           0.01867\n84358402   94.44      0.011490        0.02461      0.05688           0.01885\n843786     27.19      0.007510        0.03345      0.03672           0.01137\n         symmetry_se fractal_dimension_se radius_worst texture_worst\n842302       0.03003             0.006193        25.38         17.33\n842517       0.01389             0.003532        24.99         23.41\n84300903     0.02250             0.004571        23.57         25.53\n84348301     0.05963             0.009208        14.91         26.50\n84358402     0.01756             0.005115        22.54         16.67\n843786       0.02165             0.005082        15.47         23.75\n         perimeter_worst area_worst smoothness_worst compactness_worst\n842302            184.60     2019.0           0.1622            0.6656\n842517            158.80     1956.0           0.1238            0.1866\n84300903          152.50     1709.0           0.1444            0.4245\n84348301           98.87      567.7           0.2098            0.8663\n84358402          152.20     1575.0           0.1374            0.2050\n843786            103.40      741.6           0.1791            0.5249\n         concavity_worst concave.points_worst symmetry_worst\n842302            0.7119               0.2654         0.4601\n842517            0.2416               0.1860         0.2750\n84300903          0.4504               0.2430         0.3613\n84348301          0.6869               0.2575         0.6638\n84358402          0.4000               0.1625         0.2364\n843786            0.5355               0.1741         0.3985\n         fractal_dimension_worst\n842302                   0.11890\n842517                   0.08902\n84300903                 0.08758\n84348301                 0.17300\n84358402                 0.07678\n843786                   0.12440\n```\n\n\n:::\n:::\n\n\n\nThe first column `diagnosis` is the expert opinion on the sample (i.e. patient FNA)\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.df$diagnosis\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n [19] \"M\" \"B\" \"B\" \"B\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n [37] \"M\" \"B\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\"\n [55] \"M\" \"B\" \"M\" \"M\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"M\" \"M\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\"\n [73] \"M\" \"M\" \"B\" \"M\" \"B\" \"M\" \"M\" \"B\" \"B\" \"B\" \"M\" \"M\" \"B\" \"M\" \"M\" \"M\" \"B\" \"B\"\n [91] \"B\" \"M\" \"B\" \"B\" \"M\" \"M\" \"B\" \"B\" \"B\" \"M\" \"M\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\"\n[109] \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"M\" \"M\" \"B\" \"M\" \"M\" \"B\" \"B\" \"B\"\n[127] \"M\" \"M\" \"B\" \"M\" \"B\" \"M\" \"M\" \"B\" \"M\" \"M\" \"B\" \"B\" \"M\" \"B\" \"B\" \"M\" \"B\" \"B\"\n[145] \"B\" \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"M\"\n[163] \"M\" \"B\" \"M\" \"B\" \"B\" \"M\" \"M\" \"B\" \"B\" \"M\" \"M\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\"\n[181] \"M\" \"M\" \"M\" \"B\" \"M\" \"B\" \"M\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\" \"M\" \"M\" \"B\" \"M\" \"M\"\n[199] \"M\" \"M\" \"B\" \"M\" \"M\" \"M\" \"B\" \"M\" \"B\" \"M\" \"B\" \"B\" \"M\" \"B\" \"M\" \"M\" \"M\" \"M\"\n[217] \"B\" \"B\" \"M\" \"M\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"M\" \"B\" \"B\" \"M\"\n[235] \"B\" \"B\" \"M\" \"M\" \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\"\n[253] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"B\" \"B\" \"B\" \"B\"\n[271] \"B\" \"B\" \"M\" \"B\" \"M\" \"B\" \"B\" \"M\" \"B\" \"B\" \"M\" \"B\" \"M\" \"M\" \"B\" \"B\" \"B\" \"B\"\n[289] \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\" \"M\" \"B\" \"M\" \"B\" \"B\" \"B\"\n[307] \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\" \"B\" \"M\" \"B\" \"M\"\n[325] \"B\" \"B\" \"B\" \"B\" \"M\" \"M\" \"M\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"M\" \"B\" \"M\" \"B\" \"B\"\n[343] \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"M\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\"\n[361] \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"M\" \"B\" \"M\" \"M\" \"M\" \"B\" \"M\" \"M\" \"B\" \"B\" \"B\" \"B\"\n[379] \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\" \"M\" \"M\" \"B\" \"B\"\n[397] \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\"\n[415] \"M\" \"B\" \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\"\n[433] \"M\" \"M\" \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\" \"M\" \"B\" \"M\" \"B\" \"B\" \"M\"\n[451] \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\"\n[469] \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\"\n[487] \"B\" \"M\" \"B\" \"M\" \"B\" \"B\" \"M\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"M\" \"B\" \"M\" \"B\" \"M\"\n[505] \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"B\" \"M\" \"B\" \"M\" \"B\" \"M\" \"M\" \"B\" \"B\" \"B\" \"M\"\n[523] \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"M\" \"B\" \"M\" \"M\" \"B\" \"B\" \"B\"\n[541] \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\"\n[559] \"B\" \"B\" \"B\" \"B\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"B\"\n```\n\n\n:::\n:::\n\n\n\n-1 is used to remove the diagnosis column, which is not needed right now, but needed later as a vector\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.data <- wisc.df[,-1]\ndiagnosis <- wisc.df[,1]\n```\n:::\n\n\nLastly, explore and get familiar \n\n>Q1. Q1. How many observations are in this dataset?\n\nThere are 569 observations/patients in the dataset.\n\n\n>Q2. How many of the observations have a malignant diagnosis?\n\nThere are 212 observations that have a malignant diagnosis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(wisc.df$diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  B   M \n357 212 \n```\n\n\n:::\n:::\n\n\n>Q3. How many variables/features in the data are suffixed with _mean?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(wisc.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"radius_mean\"             \"texture_mean\"           \n [3] \"perimeter_mean\"          \"area_mean\"              \n [5] \"smoothness_mean\"         \"compactness_mean\"       \n [7] \"concavity_mean\"          \"concave.points_mean\"    \n [9] \"symmetry_mean\"           \"fractal_dimension_mean\" \n[11] \"radius_se\"               \"texture_se\"             \n[13] \"perimeter_se\"            \"area_se\"                \n[15] \"smoothness_se\"           \"compactness_se\"         \n[17] \"concavity_se\"            \"concave.points_se\"      \n[19] \"symmetry_se\"             \"fractal_dimension_se\"   \n[21] \"radius_worst\"            \"texture_worst\"          \n[23] \"perimeter_worst\"         \"area_worst\"             \n[25] \"smoothness_worst\"        \"compactness_worst\"      \n[27] \"concavity_worst\"         \"concave.points_worst\"   \n[29] \"symmetry_worst\"          \"fractal_dimension_worst\"\n```\n\n\n:::\n\n```{.r .cell-code}\nlength(grep(\"_mean\", colnames(wisc.data)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10\n```\n\n\n:::\n:::\n\n## Principal Coordinate Analysis \n\nThe `prcomp()` function to do PCA has a `scale=FALSE`default. In general we nearly always want to set this to TRUE so our analysis is not dominated by columns/variables in our data set that have high standard deviation and mean when compared to others just because the units of measurement are on different scales \n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.pr <- prcomp( wisc.data,scale=TRUE)\nsummary(wisc.pr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     3.6444 2.3857 1.67867 1.40735 1.28403 1.09880 0.82172\nProportion of Variance 0.4427 0.1897 0.09393 0.06602 0.05496 0.04025 0.02251\nCumulative Proportion  0.4427 0.6324 0.72636 0.79239 0.84734 0.88759 0.91010\n                           PC8    PC9    PC10   PC11    PC12    PC13    PC14\nStandard deviation     0.69037 0.6457 0.59219 0.5421 0.51104 0.49128 0.39624\nProportion of Variance 0.01589 0.0139 0.01169 0.0098 0.00871 0.00805 0.00523\nCumulative Proportion  0.92598 0.9399 0.95157 0.9614 0.97007 0.97812 0.98335\n                          PC15    PC16    PC17    PC18    PC19    PC20   PC21\nStandard deviation     0.30681 0.28260 0.24372 0.22939 0.22244 0.17652 0.1731\nProportion of Variance 0.00314 0.00266 0.00198 0.00175 0.00165 0.00104 0.0010\nCumulative Proportion  0.98649 0.98915 0.99113 0.99288 0.99453 0.99557 0.9966\n                          PC22    PC23   PC24    PC25    PC26    PC27    PC28\nStandard deviation     0.16565 0.15602 0.1344 0.12442 0.09043 0.08307 0.03987\nProportion of Variance 0.00091 0.00081 0.0006 0.00052 0.00027 0.00023 0.00005\nCumulative Proportion  0.99749 0.99830 0.9989 0.99942 0.99969 0.99992 0.99997\n                          PC29    PC30\nStandard deviation     0.02736 0.01153\nProportion of Variance 0.00002 0.00000\nCumulative Proportion  1.00000 1.00000\n```\n\n\n:::\n:::\n\n\nThe main PC result figure is called a \"score plot\" or a \"PC polt\" or \"ordination plot\"...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nggplot(wisc.pr$x)+\n  aes(PC1, PC2, col=diagnosis) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](class08_miniproject_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?\n\n44.27% of the original variance is captured by PC1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(wisc.pr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     3.6444 2.3857 1.67867 1.40735 1.28403 1.09880 0.82172\nProportion of Variance 0.4427 0.1897 0.09393 0.06602 0.05496 0.04025 0.02251\nCumulative Proportion  0.4427 0.6324 0.72636 0.79239 0.84734 0.88759 0.91010\n                           PC8    PC9    PC10   PC11    PC12    PC13    PC14\nStandard deviation     0.69037 0.6457 0.59219 0.5421 0.51104 0.49128 0.39624\nProportion of Variance 0.01589 0.0139 0.01169 0.0098 0.00871 0.00805 0.00523\nCumulative Proportion  0.92598 0.9399 0.95157 0.9614 0.97007 0.97812 0.98335\n                          PC15    PC16    PC17    PC18    PC19    PC20   PC21\nStandard deviation     0.30681 0.28260 0.24372 0.22939 0.22244 0.17652 0.1731\nProportion of Variance 0.00314 0.00266 0.00198 0.00175 0.00165 0.00104 0.0010\nCumulative Proportion  0.98649 0.98915 0.99113 0.99288 0.99453 0.99557 0.9966\n                          PC22    PC23   PC24    PC25    PC26    PC27    PC28\nStandard deviation     0.16565 0.15602 0.1344 0.12442 0.09043 0.08307 0.03987\nProportion of Variance 0.00091 0.00081 0.0006 0.00052 0.00027 0.00023 0.00005\nCumulative Proportion  0.99749 0.99830 0.9989 0.99942 0.99969 0.99992 0.99997\n                          PC29    PC30\nStandard deviation     0.02736 0.01153\nProportion of Variance 0.00002 0.00000\nCumulative Proportion  1.00000 1.00000\n```\n\n\n:::\n:::\n\n\n\n> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?\n\n3 principal components are required to describe at least 70% of the data \n\n> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?\n\nAbout 8 principal components are requied to describe at least 90% of the original variance in the data \n\n\n## Interpreting PCA Results\n\nCreate a biplot of the `wisc.pr` using the `biplot()` function \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbiplot(wisc.pr)\n```\n\n::: {.cell-output-display}\n![](class08_miniproject_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n> Q7. What stand out to you about this plot? Is it easy or difficult to understand and why?\n\nThere are too many plots on this plot, there is no scaling of values to remove this noise. \n\n> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots? \n\nThese plots show a distinct difference in clustering between the two colors. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncol <- ifelse(diagnosis == \"M\", \"red\", \"blue\")\nplot(wisc.pr$x[,c(1,3)], col = col , \n     xlab = \"PC1\", ylab = \"PC3\")\n```\n\n::: {.cell-output-display}\n![](class08_miniproject_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## PCA Scree-plot \n\n\nA plot of how much variance each PC captures. We can get this from `wisc.pr$sdev` or from the output of `summary(wisc.pr)`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar.tbl <- summary(wisc.pr)\nhead(var.tbl$importance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            PC1      PC2      PC3      PC4      PC5      PC6\nStandard deviation     3.644394 2.385656 1.678675 1.407352 1.284029 1.098798\nProportion of Variance 0.442720 0.189710 0.093930 0.066020 0.054960 0.040250\nCumulative Proportion  0.442720 0.632430 0.726360 0.792390 0.847340 0.887590\n                             PC7       PC8       PC9      PC10      PC11\nStandard deviation     0.8217178 0.6903746 0.6456739 0.5921938 0.5421399\nProportion of Variance 0.0225100 0.0158900 0.0139000 0.0116900 0.0098000\nCumulative Proportion  0.9101000 0.9259800 0.9398800 0.9515700 0.9613700\n                            PC12      PC13      PC14      PC15      PC16\nStandard deviation     0.5110395 0.4912815 0.3962445 0.3068142 0.2826001\nProportion of Variance 0.0087100 0.0080500 0.0052300 0.0031400 0.0026600\nCumulative Proportion  0.9700700 0.9781200 0.9833500 0.9864900 0.9891500\n                            PC17      PC18      PC19      PC20      PC21\nStandard deviation     0.2437192 0.2293878 0.2224356 0.1765203 0.1731268\nProportion of Variance 0.0019800 0.0017500 0.0016500 0.0010400 0.0010000\nCumulative Proportion  0.9911300 0.9928800 0.9945300 0.9955700 0.9965700\n                            PC22      PC23      PC24      PC25      PC26\nStandard deviation     0.1656484 0.1560155 0.1343689 0.1244238 0.0904303\nProportion of Variance 0.0009100 0.0008100 0.0006000 0.0005200 0.0002700\nCumulative Proportion  0.9974900 0.9983000 0.9989000 0.9994200 0.9996900\n                             PC27      PC28       PC29       PC30\nStandard deviation     0.08306903 0.0398665 0.02736427 0.01153451\nProportion of Variance 0.00023000 0.0000500 0.00002000 0.00000000\nCumulative Proportion  0.99992000 0.9999700 1.00000000 1.00000000\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npr.var <- wisc.pr$sdev^2\nhead(pr.var)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 13.281608  5.691355  2.817949  1.980640  1.648731  1.207357\n```\n\n\n:::\n:::\n\n\nVariance explained by each principal component: pve\n\n::: {.cell}\n\n```{.r .cell-code}\npve <- pr.var / sum(pr.var)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(pve, xlab = \"Principal Component\", \n     ylab = \"Proportion of Variance Explained\", \n     ylim = c(0, 1), type = \"o\")\n```\n\n::: {.cell-output-display}\n![](class08_miniproject_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nAn alternative scree plot can be made \n\n\n::: {.cell}\n\n```{.r .cell-code}\nscreeplot <- barplot(pve, ylab = \"Precent of Variance Explained\",\n     names.arg=paste0(\"PC\",1:length(pve)), las=2, axes = FALSE)\naxis(2, at=pve, labels=round(pve,2)*100 )\n```\n\n::: {.cell-output-display}\n![](class08_miniproject_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n> Q9. For the first principal component, what is the component of the loading vector (i.e. `wisc.pr$rotation[,1]`) for the feature `concave.points_mean`?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.pr$rotation[\"concave.points_mean\", 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.2608538\n```\n\n\n:::\n:::\n\n\n> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?\n\n5 principal components are required to explain 80% of the variance of the data \n\n\n\n\n## Hierarchical clustering \n\nJust clusting the original data is not very informative or helpful \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.scaled <- scale(wisc.data)\ndata.dist <- dist(data.scaled)\nwisc.hclust <- hclust (data.dist)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot (wisc.hclust)\n```\n\n::: {.cell-output-display}\n![](class08_miniproject_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.hclust.clusters <- cutree(wisc.hclust, k=4)\ntable(cutree(wisc.hclust, k=4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  1   2   3   4 \n177   7 383   2 \n```\n\n\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(wisc.hclust.clusters,diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    diagnosis\nwisc.hclust.clusters   B   M\n                   1  12 165\n                   2   2   5\n                   3 343  40\n                   4   0   2\n```\n\n\n:::\n:::\n\n\n\n\n\n## Combining methods (PCA and Clustering)\n\nClustering the original data was not very productive. The PCA results looked promising. Here we combined these methods by clustering from our PCA results. In other words \"clustering in PC space\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndist.pc <- dist(wisc.pr$x[,1:3])\nwisc.pr.hclust <- hclust(dist.pc, method = \"ward.D2\")\n```\n:::\n\n\n\n> Q11. Using the `plot()` and `abline()` functions, what is the height at which the clustering model has four clusters?\n\nheight=35 is when the clustering model of `wisc.pr.hclust` has four clusters. \n\nView the tree... \n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(wisc.pr.hclust)\nabline(h=35, col=\"red\")\n```\n\n::: {.cell-output-display}\n![](class08_miniproject_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nTo get out clustering membership vector (i.e. out main clustering result) we \"cut\" the tree at a desired height or to yield a desired number of \"k\" \n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrps <- cutree(wisc.pr.hclust, h=70)\ntable(grps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ngrps\n  1   2 \n203 366 \n```\n\n\n:::\n:::\n\nHow does this clustering grps compare to the expert diagnosis \n\n::: {.cell}\n\n```{.r .cell-code}\ntable(grps, diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    diagnosis\ngrps   B   M\n   1  24 179\n   2 333  33\n```\n\n\n:::\n:::\n\n> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10? \n\nYes, because there is less split of the data. When there is two clusters there is a split in data where there is a majority of M in one and B in the other. \n\n> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.\n\nUsing `ward.D2` provides a clear separation between M and B samples that were both balanced. Overall, M and B were accurately grouped and a strong alignment in each cluster can be observed. \n\n## Combining Methods \n\nCreate a graph of PC1 and PC2 \n\n::: {.cell}\n\n```{.r .cell-code}\ncol <- ifelse(diagnosis == \"M\", \"red\", \"black\")\nplot(wisc.pr$x[,1:2], col=col)\n```\n\n::: {.cell-output-display}\n![](class08_miniproject_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nRe-order the factor and re plot \n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- as.factor(grps)\nlevels(g)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"1\" \"2\"\n```\n\n\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- relevel(g,2)\nlevels(g)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"2\" \"1\"\n```\n\n\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(wisc.pr$x[,1:2], col=g)\n```\n\n::: {.cell-output-display}\n![](class08_miniproject_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\nCut this model into two clusters and assign to `wisc.pr.hclust.clusters`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)\ntable(wisc.pr.hclust.clusters, diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                       diagnosis\nwisc.pr.hclust.clusters   B   M\n                      1  24 179\n                      2 333  33\n```\n\n\n:::\n:::\n\n> Q15. How well does the newly created model with four clusters separate out the two diagnoses?\n\nThere is a distinct and balaned separation of the four clusters. Cluster 1 consist of majority M samples and cluster 2 consists of majority B samples. \n\n> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.\n\nThe k means model separates the diagnoses well however there is some overlap. The heiarchical model shows a distinct separation but with four clusters instead of 2, thus fragmenting the separation. \n\n\nQ17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity? \n\nHierarchical clustering is both higher in sensitivity and specificity than the k means model. This was concluded as there were greater benign and malignant samples that were correctly grouped. \n\nSensitivity: TP/(TP+FN)\nSpecificity: TN/(TN+FN)\n\n## 7. Prediction \n\nWe can use out PCA model for prediction with new input patient samples. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#url <- \"new_samples.csv\"\nurl <- \"https://tinyurl.com/new-samples-CSV\"\nnew <- read.csv(url)\nnpc <- predict(wisc.pr, newdata=new)\nnpc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           PC1       PC2        PC3        PC4       PC5        PC6        PC7\n[1,]  2.576616 -3.135913  1.3990492 -0.7631950  2.781648 -0.8150185 -0.3959098\n[2,] -4.754928 -3.009033 -0.1660946 -0.6052952 -1.140698 -1.2189945  0.8193031\n            PC8       PC9       PC10      PC11      PC12      PC13     PC14\n[1,] -0.2307350 0.1029569 -0.9272861 0.3411457  0.375921 0.1610764 1.187882\n[2,] -0.3307423 0.5281896 -0.4855301 0.7173233 -1.185917 0.5893856 0.303029\n          PC15       PC16        PC17        PC18        PC19       PC20\n[1,] 0.3216974 -0.1743616 -0.07875393 -0.11207028 -0.08802955 -0.2495216\n[2,] 0.1299153  0.1448061 -0.40509706  0.06565549  0.25591230 -0.4289500\n           PC21       PC22       PC23       PC24        PC25         PC26\n[1,]  0.1228233 0.09358453 0.08347651  0.1223396  0.02124121  0.078884581\n[2,] -0.1224776 0.01732146 0.06316631 -0.2338618 -0.20755948 -0.009833238\n             PC27        PC28         PC29         PC30\n[1,]  0.220199544 -0.02946023 -0.015620933  0.005269029\n[2,] -0.001134152  0.09638361  0.002795349 -0.019015820\n```\n\n\n:::\n:::\n\n\n",
    "supporting": [
      "class08_miniproject_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}